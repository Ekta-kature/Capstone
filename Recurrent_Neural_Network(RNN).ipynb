{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOX5E0Vrh0ooMqj5rCCu2bX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ekta-kature/Capstone/blob/main/Recurrent_Neural_Network(RNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Recurrent Neural Network(RNN)**\n",
        "\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are a type of neural network designed for sequence data. They are particularly useful for tasks where the order of the input data is important, such as time series prediction, natural language processing, and speech recognition. Here’s an overview of how deep learning using RNNs works:\n",
        "\n",
        "## **Basics of RNNs**\n",
        "\n",
        "Sequential Data Handling: RNNs can handle sequences of data by maintaining a hidden state that is passed from one time step to the next. This allows them to capture temporal dependencies in the data.\n",
        "\n",
        "Hidden States: At each time step, the RNN takes an input and the previous hidden state to produce a new hidden state and an output.\n",
        "\n",
        "Backpropagation Through Time (BPTT): Training an RNN involves unrolling the network through time and using backpropagation to compute gradients. This process is known as BPTT."
      ],
      "metadata": {
        "id": "duCkB1j4m3_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Install Required Libraries\n",
        "\n",
        "Make sure we have the necessary libraries installed. We can install them using pip if you don't have them already."
      ],
      "metadata": {
        "id": "1DCW_hLDn4lg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l2Rsp4Gmhnd",
        "outputId": "e3dfd996-f215-4e1a-85cb-6e03649a470d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "pip install torch numpy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "m-IfXXdxotWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "8i7JVuMGo0w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Generate Dummy Data"
      ],
      "metadata": {
        "id": "BXZ07tNGo20P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dummy data\n",
        "data = np.array([[[i+j] for i in range(10)] for j in range(1000)])\n",
        "labels = np.array([[i+10] for i in range(1000)])\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "data = torch.tensor(data, dtype=torch.float32)\n",
        "labels = torch.tensor(labels, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "1fJ2k-t1o9F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Define the RNN model"
      ],
      "metadata": {
        "id": "3r8FikccpIWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), 32)  # Initial hidden state\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])  # Take the output of the last time step\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleRNN(input_size=1, hidden_size=32, output_size=1)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "NHSnTsVlpMld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Train the model"
      ],
      "metadata": {
        "id": "aBzKYgcUpUOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        inputs = data[i:i+batch_size]\n",
        "        targets = labels[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrcTCEdOpakH",
        "outputId": "faac02bc-2d26-41a9-e6b8-02d1ec7a7453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1006534.2500\n",
            "Epoch [2/10], Loss: 1004545.7500\n",
            "Epoch [3/10], Loss: 1002623.3125\n",
            "Epoch [4/10], Loss: 1000731.9375\n",
            "Epoch [5/10], Loss: 998859.0000\n",
            "Epoch [6/10], Loss: 996998.5000\n",
            "Epoch [7/10], Loss: 995148.0000\n",
            "Epoch [8/10], Loss: 993305.5625\n",
            "Epoch [9/10], Loss: 991470.2500\n",
            "Epoch [10/10], Loss: 989641.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.Make Predictions"
      ],
      "metadata": {
        "id": "G5gQIRx5peXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "test_data = np.array([[[i+j] for i in range(10)] for j in range(5, 10)])\n",
        "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
        "predictions = model(test_data)\n",
        "\n",
        "# Print predictions\n",
        "for i, prediction in enumerate(predictions.detach().numpy()):\n",
        "    print(f\"Sequence {i+1}: {test_data[i].numpy().flatten()} -> Prediction: {prediction[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wZpPJfiposL",
        "outputId": "0ecfdca3-2d0c-401f-82bd-1b014d131982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence 1: [ 5.  6.  7.  8.  9. 10. 11. 12. 13. 14.] -> Prediction: 10.258186340332031\n",
            "Sequence 2: [ 6.  7.  8.  9. 10. 11. 12. 13. 14. 15.] -> Prediction: 10.32396125793457\n",
            "Sequence 3: [ 7.  8.  9. 10. 11. 12. 13. 14. 15. 16.] -> Prediction: 10.37914752960205\n",
            "Sequence 4: [ 8.  9. 10. 11. 12. 13. 14. 15. 16. 17.] -> Prediction: 10.425650596618652\n",
            "Sequence 5: [ 9. 10. 11. 12. 13. 14. 15. 16. 17. 18.] -> Prediction: 10.464993476867676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **conclusion**\n",
        "\n",
        "I implemented a basic Recurrent Neural Network (RNN) using PyTorch for sequence prediction. Key points include:\n",
        "\n",
        "**Data Preparation:** Synthetic sequences were split into training and testing sets.\n",
        "\n",
        "**Model:** A simple RNN model was trained to predict the next value in a sequence.\n",
        "\n",
        "**Training and Evaluation:** The model's performance was evaluated using loss plots and predictions on test data.\n",
        "\n",
        "**Insights: **The RNN learned to make predictions, but tuning and more complex models may improve results further.\n",
        "\n",
        "This setup provides a foundation for working with sequential data in machine learning."
      ],
      "metadata": {
        "id": "rVIjN1k7saAS"
      }
    }
  ]
}